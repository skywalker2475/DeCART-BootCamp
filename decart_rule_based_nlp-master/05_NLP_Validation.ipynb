{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import codecs\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import sklearn.metrics\n",
    "\n",
    "# and also our utilities for this class\n",
    "from nlp_pneumonia_utils import Annotation\n",
    "from nlp_pneumonia_utils import AnnotatedDocument\n",
    "from nlp_pneumonia_utils import read_brat_annotations\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from nlp_pneumonia_utils import read_annotations\n",
    "from nlp_pneumonia_utils import calculate_prediction_metrics\n",
    "from nlp_pneumonia_utils import mark_text\n",
    "from nlp_pneumonia_utils import clearPyConTextRegularExpressions\n",
    "from nlp_pneumonia_utils import pneumonia_annotation_html_markup\n",
    "print('Loaded utilities...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we'll load in our dataset but throughout these notebooks, there are a lot of utility functions used.  \n",
    "\n",
    "### (OPTIONAL) Feel free to look at them here in the repository : <a href=\"https://github.com/UUDeCART/decart_rule_based_nlp/blob/master/nlp_pneumonia_utils.py\">nlp_pneumonia_utils.py</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First thing, let's load our training set\n",
    "annotated_doc_map = read_doc_annotations('data/training_v2.zip')\n",
    "\n",
    "# let's also use a simple list of documents as well as this map\n",
    "annotated_docs = list(annotated_doc_map.values())\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "total_positives = 0\n",
    "for anno_doc in annotated_docs:\n",
    "    if anno_doc.positive_label:\n",
    "        total_positives += 1\n",
    "    \n",
    "print('Total Positive Pneumonia Documents : {0}'.format(total_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's find the document with the most annotations\n",
    "most_annotated_doc = None\n",
    "for anno_doc in annotated_docs:\n",
    "    if most_annotated_doc is None or len(anno_doc.annotations) > len(most_annotated_doc.annotations):\n",
    "        most_annotated_doc = anno_doc\n",
    "        #print('Most Annotations so far : {}'.format(len(most_annotated_doc.annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, recall annotations annotated by our expert.  Note that there are 3 total annotation types in this set : \n",
    "1. **DOCUMENT_PNEUMONIA_YES* -> Document shows **active** or **possible** case of pneumonia\n",
    "2. **DOCUMENT_PNEUMONIA_NO** -> Document shows **no evidence** of pneumonia\n",
    "3. **SPAN_POSITIVE_PNEUMONIA_EVIDENCE** -> Spans of phrases/sentence which show positive or possible evidence of pneumonia which led the expert annotator to the final document-level conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's render one of our annotated documents in HTML.  When using the function 'pneumonia_annotation_html_markup' these show up as the colors:\n",
    "1. **DOCUMENT_PNEUMONIA_YES** -> RED\n",
    "2. **DOCUMENT_PNEUMONIA_NO** -> GREEN\n",
    "3. **SPAN_POSITIVE_PNEUMONIA_EVIDENCE** -> RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's display one of our documents in HTML\n",
    "display(HTML(pneumonia_annotation_html_markup(most_annotated_doc).replace('\\n', '<br>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this course, we will work in groups to develop rule-based systems to correctly identify cases of pneumonia.\n",
    "\n",
    "## Before we do that, let's establish some baselines.  These initial baselines will be fairly naive but they will help to illustrate the principles of Precision, Recall and F1 measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's recall those formulas from the previous notebook:\n",
    "\n",
    "## Precision and Recall:\n",
    "<img src=images/precision_recall.jpg>\n",
    "\n",
    "## F1:\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/7d63c1f5c659f95b5dfe5893213cc8ea7f8bea0a>\n",
    "<p>Source: Wikipedia</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we set up a few baselines, some explanation on calculate_prediction_metrics() from our utilities file.  Its arguments are:\n",
    "1. A list of documents which have been annotated (AnnotatedDocument class in our scripts)\n",
    "2. A function which takes a string (i.e. text) and returns either a 0 or 1 (0 for NO PNEUMONIA, 1 for DEFINITE or POSSIBLE)\n",
    "\n",
    "From these predictions, it calculates Precision, Recall and F1 measure and writes a <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a>.\n",
    "\n",
    "## This function will enable your group projects, but more on that later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First, a naive baseline by always predicting NO pneumonia (i.e. 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_negative_pneumonia_prediction(text):\n",
    "    return 0\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting NO')\n",
    "calculate_prediction_metrics(annotated_docs, naive_negative_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, a naive baseline by always predicting YES pneumonia (i.e. 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_positive_pneumonia_prediction(text):\n",
    "    return 1\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting YES')\n",
    "calculate_prediction_metrics(annotated_docs, naive_positive_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then let's try a more intelligent baseline to assign positive Pneumonia anytime the word \"pneumonia\" appears in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_pneumonia_keyword_prediction(text):\n",
    "    if 'pneumonia' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print('Predicting and validating the naive PNEUMONIA keyword baseline')\n",
    "calculate_prediction_metrics(annotated_docs, naive_pneumonia_keyword_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2017.<br/>\n",
    "Presenters : Dr. Wendy Chapman, Jianlin Shi and Kelly Peterson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
